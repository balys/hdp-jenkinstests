---

#
# LOCAL STEPS on JENKINS Server
#

- name: Create a Remote Workstation and deploy Hadoop
  hosts: localhost
  connection: local
  gather_facts: False
  tasks:
    - name: Get RAX authentication token
      uri:
        url: "https://identity.api.rackspacecloud.com/v2.0/tokens"
        method: POST
        return_content: true
        status_code: 200
        body_format: json
        headers:
          "Content-Type" : "application/json"
        body:
          "auth":
            "RAX-KSKEY:apiKeyCredentials":
              "username": "{{ rax_username }}"
              "apiKey":   "{{ rax_apikey }}"
      register: rax_auth_info

    - name: Create Workstation Node in RAX Cloud
      uri:
        url: "https://{{ rax_deploy_region }}.servers.api.rackspacecloud.com/v2/{{rax_account}}/servers"
        method: POST
        return_content: true
        status_code: 202
        body_format: json
        headers:
          "Content-Type" : "application/json"
          "X-Auth-Token" : "{{ rax_auth_info.json.access.token.id }}"
        body:
          "server" :
            "name": "HDP-testing-jenkins-{{ buildidentifier }}"
            "imageRef": "{{ cloud_image }}"
            "flavorRef": "{{ cloud_flavor }}"
            "key_name": "{{ ssh_keyname }}"
      register: workstation_node_info

    - name: Pause for 2 minutes for the Workstation to come online
      pause:
        minutes: 2

    - name: Get Workstation Node Information
      uri:
        url: "https://{{ rax_deploy_region }}.servers.api.rackspacecloud.com/v2/{{rax_account}}/servers/{{ workstation_node_info.json.server.id }}"
        method: GET
        return_content: true
        status_code: 200
        headers:
          "Content-Type" : "application/json"
          "X-Auth-Token" : "{{ rax_auth_info.json.access.token.id }}"
      register: workstation_node_details

    - name: Show Workstation IP
      debug: var=workstation_node_details['json']['server']['accessIPv4']

    - name: Add workstation host to workstation_nodes ansible group
      add_host:
        name: "{{ workstation_node_details['json']['server']['accessIPv4'] }}"
        ansible_host: "{{ workstation_node_details['json']['server']['accessIPv4'] }}"
        ansible_user: root
        ansible_ssh_pass: "{{ workstation_node_info['json']['server']['adminPass'] }}"
        ansible_ssh_extra_args: "-o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no"
        groups: workstation_nodes

    - name: Clean up temp folder for repo archive
      file:
        path: /tmp/ansible-hadoop-BUILDTEST
        state: absent

    - name: Copy repo files to temp folder
      command: cp -a /var/lib/jenkins/workspace/ansible-hadoop /tmp/ansible-hadoop-BUILDTEST

    - name: Create an archive of temp repository folder
      command: tar czf /tmp/HDP-testing-jenkins-{{ buildidentifier }}.tgz /tmp/ansible-hadoop-BUILDTEST


#
# Set-up workstation
#
- name: Perform steps on workstation server
  hosts: workstation_nodes
  connection: ssh
  gather_facts: False
  tasks:
    - name: Run "w" on workstation server
      command: w
      register: outputw

    - debug: var=outputw['stdout_lines']

    - name: Upload and extract ansible-hadoop repo contents to {{ deploytempfolder }}
      unarchive:
        src: /tmp/HDP-testing-jenkins-{{ buildidentifier }}.tgz
        dest: /root

    - name: Run "ls -alh {{ deploytempfolder }}/{{ releasefolder }}" on workstation server
      command: ls -alh {{ deploytempfolder }}/{{ releasefolder }}
      register: outputfind

    - debug: var=outputfind['stdout_lines']

    - name: "Install required yum packages on workstation"
      yum: name={{ item }} state=installed
      with_flattened:
        - "python-virtualenv"
        - "python-pip"
        - "python-devel"
        - "sshpass"
        - "git"
        - "vim-enhanced"
        - "libffi"
        - "libffi-devel"
        - "gcc"
        - "openssl-devel"

    - name: "Install ansible {{ workstation_ansibleversion }} on workstation"
      pip:
        name: "ansible=={{ workstation_ansibleversion }}"

    - name: "Install pyrax on workstation"
      pip:
        name: "pyrax"

    - name: "Set up pyrax credentials file"
      template: src=raxpub.j2 dest={{ rax_credentials_file }}

    - name: "Make changes in {{ deploytempfolder }}/{{ releasefolder }}/playbooks/group_vars/all.."
      command: sed -i "s/hadoop-ssh-key/{{ ssh_keyname }}/g" {{ deploytempfolder }}/{{ releasefolder }}/playbooks/group_vars/all
    - command: sed -i "s/localnet/{{ buildidentifier }}/g" {{ deploytempfolder }}/{{ releasefolder }}/playbooks/group_vars/all
    - command: sed -i "s/rax_region{{ ":" }} 'ORD'/rax_region{{ ":" }} '{{ rax_deploy_region }}' /g" {{ deploytempfolder }}/{{ releasefolder }}/playbooks/group_vars/all
    - command: sed -i "s/use_dns{{ ":" }} true/use_dns{{ ":" }} false/g" {{ deploytempfolder }}/{{ releasefolder }}/playbooks/group_vars/all
    - command: sed -i "s/distro{{ ":" }} 'hdp'/distro{{ ":" }} '{{ hadoop_build }}'/g" {{ deploytempfolder }}/{{ releasefolder }}/playbooks/group_vars/all

    - name: "Update the {{ hadoop_build }} version to {{ hadoop_version }} in {{ deploytempfolder }}/{{ releasefolder }}/playbooks/group_vars/hortonworks"
      command: sed -i "s/hdp_version{{ ":" }} '2.5'/hdp_version{{ ":" }} '{{ hadoop_version }}'/g" {{ deploytempfolder }}/{{ releasefolder }}/playbooks/group_vars/hortonworks

   # We are not updating the CDH version as we are testing one version, which is already in the config file.

    - name: "Check contents of updated {{ deploytempfolder }}/{{ releasefolder }}/playbooks/group_vars/all file"
      command: cat {{ deploytempfolder }}/{{ releasefolder }}/playbooks/group_vars/all
      register: workstation_groupvars_all

    - debug: var=workstation_groupvars_all['stdout_lines']

    - name: "Set up {{ deploytempfolder }}/{{ releasefolder }}/playbooks/group_vars/master-nodes file"
      template: src=master-nodes.j2 dest={{ deploytempfolder }}/{{ releasefolder }}/playbooks/group_vars/master-nodes

    - name: "Set up {{ deploytempfolder }}/{{ releasefolder }}/playbooks/group_vars/slave-nodes file"
      template: src=slave-nodes.j2 dest={{ deploytempfolder }}/{{ releasefolder }}/playbooks/group_vars/slave-nodes

    - name: "Set up {{ deploytempfolder }}/{{ releasefolder }}/playbooks/group_vars/edge-nodes file"
      template: src=edge-nodes.j2 dest={{ deploytempfolder }}/{{ releasefolder }}/playbooks/group_vars/edge-nodes


    - name: Copy over the private, public keys for {{ key_location }} to workstation server, and set permissions.
      copy:
        src: "{{ key_location }}"
        dest: /root/.ssh/id_rsa
    - file:
        path: /root/.ssh/id_rsa
        mode: 0600
    - copy:
        src: "{{ key_location }}.pub"
        dest: /root/.ssh/id_rsa.pub


# Clean up any residue servers created by other runs
- include: cleaner_pre.yml


#
# Deploy the cluster
#
- name: Deploy the cluster from workstation
  hosts: workstation_nodes
  connection: ssh
  gather_facts: False
  tasks:
    - name: Run provision_rax.sh on workstation server
      command: /bin/bash provision_rax.sh
      args:
        chdir: "{{ deploytempfolder }}/{{ releasefolder }}"
      register: workstation_provision_rax

    - debug: var=workstation_provision_rax['stdout_lines']


    - name: Run bootstrap_rax.sh on workstation server
      command: /bin/bash bootstrap_rax.sh
      args:
        chdir: "{{ deploytempfolder }}/{{ releasefolder }}"
      register: workstation_bootstrap_rax

    - debug: var=workstation_bootstrap_rax['stdout_lines']


    - name: Run hortonworks_rax.sh on workstation server
      command: /bin/bash hortonworks_rax.sh
      args:
        chdir: "{{ deploytempfolder }}/{{ releasefolder }}"
      register: workstation_hortonworks_rax

    - debug: var=workstation_hortonworks_rax['stdout_lines']


# Remove the servers created for this deployment
- include: cleaner_post.yml

